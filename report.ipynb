{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "321b82df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_metrics(metrics_path):\n",
    "    \"\"\"Carga el archivo metrics.json\"\"\"\n",
    "    if not os.path.exists(metrics_path):\n",
    "        raise FileNotFoundError(f\"No se encontrÃ³ el archivo: {metrics_path}\")\n",
    "    with open(metrics_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def compare_models(mlp_metrics_path, cnn_metrics_path, experiment_name=\"Experiment\", save_dir=\"./results\", export_latex=True):\n",
    "    \"\"\"\n",
    "    Compara dos modelos (MLP vs CNN) usando sus archivos metrics.json.\n",
    "    \n",
    "    ParÃ¡metros:\n",
    "    -----------\n",
    "    mlp_metrics_path : str\n",
    "        Ruta al metrics.json del MLP.\n",
    "    cnn_metrics_path : str\n",
    "        Ruta al metrics.json del CNN.\n",
    "    experiment_name : str\n",
    "        Nombre del experimento (para etiquetar resultados).\n",
    "    save_dir : str\n",
    "        Carpeta donde se guardarÃ¡ el CSV y el LaTeX (por defecto ./results).\n",
    "    export_latex : bool\n",
    "        Si True, exporta la tabla comparativa tambiÃ©n en formato LaTeX.\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Cargar ambos archivos\n",
    "    mlp_metrics = load_metrics(mlp_metrics_path)\n",
    "    cnn_metrics = load_metrics(cnn_metrics_path)\n",
    "\n",
    "    # Crear DataFrame con mÃ©tricas globales comparables\n",
    "    data = [\n",
    "        {\n",
    "            'Experiment': experiment_name,\n",
    "            'Model': 'MLP',\n",
    "            'Test Accuracy': mlp_metrics.get('test_accuracy'),\n",
    "            'Test Loss': mlp_metrics.get('test_loss'),\n",
    "            'Precision': mlp_metrics.get('macro_precision'),\n",
    "            'Recall': mlp_metrics.get('macro_recall'),\n",
    "            'F1 Score': mlp_metrics.get('macro_f1'),\n",
    "            'Trainable Params': mlp_metrics.get('trainable_parameters'),\n",
    "            'Total Params': mlp_metrics.get('total_parameters'),\n",
    "            'Inference Time (s)': mlp_metrics.get('inference_time'),\n",
    "            'Inference Speed (samples/s)': mlp_metrics.get('inference_speed'),\n",
    "            'Final Epoch': mlp_metrics.get('final_epoch'),\n",
    "            'Mean Epoch Time (s)': mlp_metrics.get('Average time per epoch'),\n",
    "            'Total Training Time (s)': mlp_metrics.get('total_training_time'),\n",
    "            'Best Val Accuracy': mlp_metrics.get('best_val_acc'),\n",
    "        },\n",
    "        {\n",
    "            'Experiment': experiment_name,\n",
    "            'Model': 'CNN',\n",
    "            'Test Accuracy': cnn_metrics.get('test_accuracy'),\n",
    "            'Test Loss': cnn_metrics.get('test_loss'),\n",
    "            'Precision': cnn_metrics.get('macro_precision'),\n",
    "            'Recall': cnn_metrics.get('macro_recall'),\n",
    "            'F1 Score': cnn_metrics.get('macro_f1'),\n",
    "            'Trainable Params': cnn_metrics.get('trainable_parameters'),\n",
    "            'Total Params': cnn_metrics.get('total_parameters'),\n",
    "            'Inference Time (s)': cnn_metrics.get('inference_time'),\n",
    "            'Inference Speed (samples/s)': cnn_metrics.get('inference_speed'),\n",
    "            'Final Epoch': cnn_metrics.get('final_epoch'),\n",
    "            'Mean Epoch Time (s)': cnn_metrics.get('Average time per epoch'),\n",
    "            'Total Training Time (s)': cnn_metrics.get('total_training_time'),\n",
    "            'Best Val Accuracy': cnn_metrics.get('best_val_acc'),\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Guardar CSV\n",
    "    csv_path = os.path.join(save_dir, f\"{experiment_name}_comparison.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"âœ… Tabla comparativa guardada en: {csv_path}\")\n",
    "\n",
    "    # Exportar a LaTeX si se desea\n",
    "    if export_latex:\n",
    "        latex_path = os.path.join(save_dir, f\"{experiment_name}_comparison.tex\")\n",
    "        with open(latex_path, \"w\") as f:\n",
    "            f.write(df.to_latex(index=False, float_format=\"%.4f\", caption=f\"ComparaciÃ³n de mÃ©tricas para {experiment_name}\", label=f\"tab:{experiment_name.lower()}\"))\n",
    "        print(f\"ðŸ“„ Tabla LaTeX guardada en: {latex_path}\")\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "367da60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_metrics(metrics_path):\n",
    "    if not os.path.exists(metrics_path):\n",
    "        raise FileNotFoundError(f\"No se encontrÃ³ el archivo: {metrics_path}\")\n",
    "    with open(metrics_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def compare_per_class_metrics(mlp_metrics_path, cnn_metrics_path, experiment_name=\"Experiment\",\n",
    "                              class_names=None, save_dir=\"./results\", export_latex=True):\n",
    "    \"\"\"\n",
    "    Compara precisiÃ³n, recall y f1 por clase entre MLP y CNN, soportando tanto listas como diccionarios.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    mlp_metrics = load_metrics(mlp_metrics_path)\n",
    "    cnn_metrics = load_metrics(cnn_metrics_path)\n",
    "\n",
    "    for key in [\"precision_per_class\", \"recall_per_class\", \"f1_per_class\"]:\n",
    "        if key not in mlp_metrics or key not in cnn_metrics:\n",
    "            raise KeyError(f\"El archivo metrics.json debe contener '{key}' para ambos modelos.\")\n",
    "\n",
    "    # Detectar si son listas o diccionarios\n",
    "    if isinstance(mlp_metrics[\"precision_per_class\"], list):\n",
    "        if class_names is None:\n",
    "            raise ValueError(\"Debes pasar la lista de class_names si las mÃ©tricas por clase estÃ¡n en formato de lista.\")\n",
    "        if len(class_names) != len(mlp_metrics[\"precision_per_class\"]):\n",
    "            raise ValueError(\"El nÃºmero de class_names no coincide con el tamaÃ±o de las mÃ©tricas.\")\n",
    "    else:\n",
    "        class_names = list(mlp_metrics[\"precision_per_class\"].keys())\n",
    "\n",
    "    # Construir DataFrame\n",
    "    rows = []\n",
    "    for i, cls in enumerate(class_names):\n",
    "        if isinstance(mlp_metrics[\"precision_per_class\"], list):\n",
    "            rows.append({\n",
    "                \"Experiment\": experiment_name,\n",
    "                \"Class\": cls,\n",
    "                \"MLP Precision\": mlp_metrics[\"precision_per_class\"][i],\n",
    "                \"MLP Recall\": mlp_metrics[\"recall_per_class\"][i],\n",
    "                \"MLP F1\": mlp_metrics[\"f1_per_class\"][i],\n",
    "                \"CNN Precision\": cnn_metrics[\"precision_per_class\"][i],\n",
    "                \"CNN Recall\": cnn_metrics[\"recall_per_class\"][i],\n",
    "                \"CNN F1\": cnn_metrics[\"f1_per_class\"][i],\n",
    "            })\n",
    "        else:\n",
    "            rows.append({\n",
    "                \"Experiment\": experiment_name,\n",
    "                \"Class\": cls,\n",
    "                \"MLP Precision\": mlp_metrics[\"precision_per_class\"].get(cls, None),\n",
    "                \"MLP Recall\": mlp_metrics[\"recall_per_class\"].get(cls, None),\n",
    "                \"MLP F1\": mlp_metrics[\"f1_per_class\"].get(cls, None),\n",
    "                \"CNN Precision\": cnn_metrics[\"precision_per_class\"].get(cls, None),\n",
    "                \"CNN Recall\": cnn_metrics[\"recall_per_class\"].get(cls, None),\n",
    "                \"CNN F1\": cnn_metrics[\"f1_per_class\"].get(cls, None),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Guardar CSV\n",
    "    csv_path = os.path.join(save_dir, f\"{experiment_name}_per_class_metrics.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"âœ… MÃ©tricas por clase guardadas en: {csv_path}\")\n",
    "\n",
    "    # Exportar LaTeX\n",
    "    if export_latex:\n",
    "        latex_path = os.path.join(save_dir, f\"{experiment_name}_per_class_metrics.tex\")\n",
    "        with open(latex_path, \"w\") as f:\n",
    "            f.write(df.to_latex(index=False, float_format=\"%.4f\",\n",
    "                                caption=f\"PrecisiÃ³n, Recall y F1 por clase para {experiment_name}\",\n",
    "                                label=f\"tab:{experiment_name.lower().replace(' ', '_')}_perclass\"))\n",
    "        print(f\"ðŸ“„ Tabla LaTeX guardada en: {latex_path}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# ---------------------\n",
    "# ðŸ”§ Ejemplo de uso\n",
    "# ---------------------\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df27da",
   "metadata": {},
   "source": [
    "# MLP y CNN without augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a03fffcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparative plots saved in ./comparative_plots_noAUG\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def load_history(history_path):\n",
    "    \"\"\"Carga history.json y retorna diccionario\"\"\"\n",
    "    with open(history_path, 'r') as f:\n",
    "        history = json.load(f)\n",
    "    return history\n",
    "\n",
    "def pad_histories(hist1, hist2):\n",
    "    \"\"\"Crea listas de la misma longitud, usando np.nan donde no hay datos\"\"\"\n",
    "    len1 = len(hist1['train_loss'])\n",
    "    len2 = len(hist2['train_loss'])\n",
    "    max_len = max(len1, len2)\n",
    "\n",
    "    def pad(lst, max_len):\n",
    "        padded = lst + [np.nan]*(max_len - len(lst))\n",
    "        return padded\n",
    "\n",
    "    padded_hist1 = {k: pad(v, max_len) if isinstance(v, list) else v for k, v in hist1.items()}\n",
    "    padded_hist2 = {k: pad(v, max_len) if isinstance(v, list) else v for k, v in hist2.items()}\n",
    "\n",
    "    return padded_hist1, padded_hist2, max_len\n",
    "\n",
    "def plot_comparative_curves(mlp_history_path, cnn_history_path, save_dir, exp_name):\n",
    "    # Cargar histories\n",
    "    mlp_hist = load_history(mlp_history_path)\n",
    "    cnn_hist = load_history(cnn_history_path)\n",
    "\n",
    "    # Padding para que tengan la misma longitud, con huecos donde falten datos\n",
    "    mlp_hist, cnn_hist, epochs = pad_histories(mlp_hist, cnn_hist)\n",
    "    x = range(1, epochs + 1)\n",
    "\n",
    "    # --- Loss ---\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(x, mlp_hist['train_loss'], label='MLP Training', color='blue', linestyle='-')\n",
    "    plt.plot(x, mlp_hist['val_loss'], label='MLP Validation', color='blue', linestyle='--')\n",
    "    plt.plot(x, cnn_hist['train_loss'], label='CNN Training', color='red', linestyle='-')\n",
    "    plt.plot(x, cnn_hist['val_loss'], label='CNN Validation', color='red', linestyle='--')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Loss Comparison - {exp_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(save_dir, f'{exp_name}_loss_comparison.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # --- Accuracy ---\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(x, mlp_hist['train_acc'], label='MLP Training', color='blue', linestyle='-')\n",
    "    plt.plot(x, mlp_hist['val_acc'], label='MLP Validation', color='blue', linestyle='--')\n",
    "    plt.plot(x, cnn_hist['train_acc'], label='CNN Training', color='red', linestyle='-')\n",
    "    plt.plot(x, cnn_hist['val_acc'], label='CNN Validation', color='red', linestyle='--')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title(f'Accuracy Comparison - {exp_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_dir, f'{exp_name}_accuracy_comparison.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Comparative plots saved in {save_dir}\")\n",
    "\n",
    "\n",
    "mlp_path = './experiments_noAUG/MLP_128_dr0.0_augFalse_20251031_134627/history.json'\n",
    "cnn_path = './experiments_noAUG/CNN_128_dr0.0_augFalse_20251031_134627/history.json'\n",
    "save_dir = './comparative_plots_noAUG'\n",
    "exp_name = 'No Augmentation'\n",
    "\n",
    "plot_comparative_curves(mlp_path, cnn_path, save_dir, exp_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b9e65612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tabla comparativa guardada en: ./comparative_tables_noAUG/No Augmentation_comparison.csv\n",
      "ðŸ“„ Tabla LaTeX guardada en: ./comparative_tables_noAUG/No Augmentation_comparison.tex\n",
      "        Experiment Model  Test Accuracy  Test Loss  Precision  Recall  \\\n",
      "0  No Augmentation   MLP         0.5198   1.524007   0.517075  0.5198   \n",
      "1  No Augmentation   CNN         0.7485   0.781191   0.759763  0.7485   \n",
      "\n",
      "   F1 Score  Trainable Params  Total Params  Inference Time (s)  \\\n",
      "0  0.515592           1738890       1738890            0.734873   \n",
      "1  0.751673            357258        357258            0.911218   \n",
      "\n",
      "   Inference Speed (samples/s)  Final Epoch  Mean Epoch Time (s)  \\\n",
      "0                 13607.792411           42             4.787476   \n",
      "1                 10974.323149           18             6.665792   \n",
      "\n",
      "   Total Training Time (s)  Best Val Accuracy  \n",
      "0               201.298620              51.96  \n",
      "1               120.045858              76.12  \n"
     ]
    }
   ],
   "source": [
    "mlp_metrics = \"./experiments_noAUG/MLP_128_dr0.0_augFalse_20251031_134627/metrics.json\"\n",
    "cnn_metrics = \"./experiments_noAUG/CNN_128_dr0.0_augFalse_20251031_134627/metrics.json\"\n",
    "\n",
    "df = compare_models(\n",
    "    mlp_metrics_path=mlp_metrics,\n",
    "    cnn_metrics_path=cnn_metrics,\n",
    "    experiment_name='No Augmentation',\n",
    "    save_dir=\"./comparative_tables_noAUG\"\n",
    ")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a6b9373b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MÃ©tricas por clase guardadas en: ./comparative_tables_noAUG/No Augmentation_per_class_metrics.csv\n",
      "ðŸ“„ Tabla LaTeX guardada en: ./comparative_tables_noAUG/No Augmentation_per_class_metrics.tex\n",
      "        Experiment       Class  MLP Precision  MLP Recall    MLP F1  \\\n",
      "0  No Augmentation    airplane       0.551259       0.613  0.580492   \n",
      "1  No Augmentation  automobile       0.626396       0.617  0.621662   \n",
      "2  No Augmentation        bird       0.465839       0.300  0.364964   \n",
      "3  No Augmentation         cat       0.366184       0.379  0.372482   \n",
      "4  No Augmentation        deer       0.444240       0.482  0.462350   \n",
      "5  No Augmentation         dog       0.440945       0.392  0.415034   \n",
      "6  No Augmentation        frog       0.538944       0.602  0.568729   \n",
      "7  No Augmentation       horse       0.586137       0.575  0.580515   \n",
      "8  No Augmentation        ship       0.574199       0.681  0.623056   \n",
      "9  No Augmentation       truck       0.576605       0.557  0.566633   \n",
      "\n",
      "   CNN Precision  CNN Recall    CNN F1  \n",
      "0       0.808026       0.745  0.775234  \n",
      "1       0.896216       0.829  0.861299  \n",
      "2       0.583537       0.716  0.643018  \n",
      "3       0.539574       0.634  0.582989  \n",
      "4       0.719298       0.697  0.707974  \n",
      "5       0.685457       0.608  0.644409  \n",
      "6       0.793265       0.848  0.819720  \n",
      "7       0.780168       0.834  0.806187  \n",
      "8       0.911188       0.790  0.846277  \n",
      "9       0.880899       0.784  0.829630  \n"
     ]
    }
   ],
   "source": [
    "df_classes = compare_per_class_metrics(\n",
    "    mlp_metrics_path=mlp_metrics,\n",
    "    cnn_metrics_path=cnn_metrics,\n",
    "    experiment_name=\"No Augmentation\",\n",
    "    class_names=class_names,\n",
    "    save_dir=\"./comparative_tables_noAUG\"\n",
    ")\n",
    "\n",
    "print(df_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401faa4c",
   "metadata": {},
   "source": [
    "# MLP and CNN with augmentation (no rotation, no colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "af85d04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparative plots saved in ./comparative_plots_AUG_2\n"
     ]
    }
   ],
   "source": [
    "mlp_path = './experiments_AUG_3/MLP_128_dr0.0_augTrue_20251102_140041/history.json'\n",
    "cnn_path = './experiments_AUG_3/CNN_128_dr0.0_augTrue_20251102_140041/history.json'\n",
    "save_dir = './comparative_plots_AUG_2'\n",
    "exp_name = 'Augmentation'\n",
    "plot_comparative_curves(mlp_path, cnn_path, save_dir, exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f949838d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tabla comparativa guardada en: ./comparative_tables_AUG_2/Augmentation_comparison.csv\n",
      "ðŸ“„ Tabla LaTeX guardada en: ./comparative_tables_AUG_2/Augmentation_comparison.tex\n",
      "     Experiment Model  Test Accuracy  Test Loss  Precision  Recall  F1 Score  \\\n",
      "0  Augmentation   MLP         0.5149   1.362507   0.514232  0.5149  0.513099   \n",
      "1  Augmentation   CNN         0.8198   0.539710   0.820326  0.8198  0.818503   \n",
      "\n",
      "   Trainable Params  Total Params  Inference Time (s)  \\\n",
      "0           1738890       1738890            0.663794   \n",
      "1            357258        357258            0.835159   \n",
      "\n",
      "   Inference Speed (samples/s)  Final Epoch  Mean Epoch Time (s)  \\\n",
      "0                 15064.913794           50            11.038301   \n",
      "1                 11973.766961           50            13.406501   \n",
      "\n",
      "   Total Training Time (s)  Best Val Accuracy  \n",
      "0               552.223622              51.52  \n",
      "1               670.514770              82.55  \n"
     ]
    }
   ],
   "source": [
    "mlp_metrics = \"./experiments_AUG_3/MLP_128_dr0.0_augTrue_20251102_140041/metrics.json\"\n",
    "cnn_metrics = \"./experiments_AUG_3/CNN_128_dr0.0_augTrue_20251102_140041/metrics.json\"\n",
    "\n",
    "df = compare_models(\n",
    "    mlp_metrics_path=mlp_metrics,\n",
    "    cnn_metrics_path=cnn_metrics,\n",
    "    experiment_name='Augmentation',\n",
    "    save_dir=\"./comparative_tables_AUG_2\"\n",
    ")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "627bc878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MÃ©tricas por clase guardadas en: ./comparative_tables_AUG_2/Augmentation_per_class_metrics.csv\n",
      "ðŸ“„ Tabla LaTeX guardada en: ./comparative_tables_AUG_2/Augmentation_per_class_metrics.tex\n",
      "     Experiment       Class  MLP Precision  MLP Recall    MLP F1  \\\n",
      "0  Augmentation    airplane       0.536323       0.598  0.565485   \n",
      "1  Augmentation  automobile       0.629828       0.587  0.607660   \n",
      "2  Augmentation        bird       0.400208       0.385  0.392457   \n",
      "3  Augmentation         cat       0.392699       0.355  0.372899   \n",
      "4  Augmentation        deer       0.421569       0.473  0.445806   \n",
      "5  Augmentation         dog       0.489089       0.381  0.428331   \n",
      "6  Augmentation        frog       0.505809       0.566  0.534214   \n",
      "7  Augmentation       horse       0.607555       0.579  0.592934   \n",
      "8  Augmentation        ship       0.619183       0.652  0.635168   \n",
      "9  Augmentation       truck       0.540057       0.573  0.556041   \n",
      "\n",
      "   CNN Precision  CNN Recall    CNN F1  \n",
      "0       0.833992       0.844  0.838966  \n",
      "1       0.954048       0.872  0.911181  \n",
      "2       0.780196       0.717  0.747264  \n",
      "3       0.690678       0.652  0.670782  \n",
      "4       0.753065       0.860  0.802988  \n",
      "5       0.786286       0.688  0.733867  \n",
      "6       0.878819       0.863  0.870838  \n",
      "7       0.852883       0.858  0.855434  \n",
      "8       0.819130       0.942  0.876279  \n",
      "9       0.854167       0.902  0.877432  \n"
     ]
    }
   ],
   "source": [
    "df_classes = compare_per_class_metrics(\n",
    "    mlp_metrics_path=mlp_metrics,\n",
    "    cnn_metrics_path=cnn_metrics,\n",
    "    experiment_name=\"Augmentation\",\n",
    "    class_names=class_names,\n",
    "    save_dir=\"./comparative_tables_AUG_2\"\n",
    ")\n",
    "\n",
    "print(df_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc8ecaf",
   "metadata": {},
   "source": [
    "# MLP & CNN - Augmentation and dropout = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "73268fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparative plots saved in ./comparative_plots_DROPOUT_2\n"
     ]
    }
   ],
   "source": [
    "mlp_path = './experiments_DROPOUT_2/MLP_128_dr0.3_augTrue_20251031_155122/history.json'\n",
    "cnn_path = './experiments_DROPOUT_2/CNN_128_dr0.3_augTrue_20251031_155122/history.json'\n",
    "save_dir = './comparative_plots_DROPOUT_2'\n",
    "exp_name = 'Augmentation & dropout = 0.3'\n",
    "plot_comparative_curves(mlp_path, cnn_path, save_dir, exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8cd38152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tabla comparativa guardada en: ./comparative_tables_DROPOUT_2/Augmentation & dropout = 0.3_comparison.csv\n",
      "ðŸ“„ Tabla LaTeX guardada en: ./comparative_tables_DROPOUT_2/Augmentation & dropout = 0.3_comparison.tex\n",
      "                     Experiment Model  Test Accuracy  Test Loss  Precision  \\\n",
      "0  Augmentation & dropout = 0.3   MLP         0.4449   1.572534   0.446506   \n",
      "1  Augmentation & dropout = 0.3   CNN         0.8104   0.548132   0.812786   \n",
      "\n",
      "   Recall  F1 Score  Trainable Params  Total Params  Inference Time (s)  \\\n",
      "0  0.4449  0.436531           1738890       1738890            0.662338   \n",
      "1  0.8104  0.808051            357258        357258            0.910115   \n",
      "\n",
      "   Inference Speed (samples/s)  Final Epoch  Mean Epoch Time (s)  \\\n",
      "0                 15098.036534           50            11.470695   \n",
      "1                 10987.622400           50            13.855877   \n",
      "\n",
      "   Total Training Time (s)  Best Val Accuracy  \n",
      "0               573.834462              44.90  \n",
      "1               692.978440              81.44  \n"
     ]
    }
   ],
   "source": [
    "mlp_metrics = \"./experiments_DROPOUT_2/MLP_128_dr0.3_augTrue_20251031_155122/metrics.json\"\n",
    "cnn_metrics = \"./experiments_DROPOUT_2/CNN_128_dr0.3_augTrue_20251031_155122/metrics.json\"\n",
    "\n",
    "df = compare_models(\n",
    "    mlp_metrics_path=mlp_metrics,\n",
    "    cnn_metrics_path=cnn_metrics,\n",
    "    experiment_name='Augmentation & dropout = 0.3',\n",
    "    save_dir=\"./comparative_tables_DROPOUT_2\"\n",
    ")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5cfbb25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MÃ©tricas por clase guardadas en: ./comparative_tables_DROPOUT_2/Augmentation & dropout = 0.3_per_class_metrics.csv\n",
      "ðŸ“„ Tabla LaTeX guardada en: ./comparative_tables_DROPOUT_2/Augmentation & dropout = 0.3_per_class_metrics.tex\n",
      "                     Experiment       Class  MLP Precision  MLP Recall  \\\n",
      "0  Augmentation & dropout = 0.3    airplane       0.511606       0.551   \n",
      "1  Augmentation & dropout = 0.3  automobile       0.558468       0.554   \n",
      "2  Augmentation & dropout = 0.3        bird       0.390593       0.191   \n",
      "3  Augmentation & dropout = 0.3         cat       0.306796       0.316   \n",
      "4  Augmentation & dropout = 0.3        deer       0.377111       0.402   \n",
      "5  Augmentation & dropout = 0.3         dog       0.438261       0.252   \n",
      "6  Augmentation & dropout = 0.3        frog       0.403874       0.563   \n",
      "7  Augmentation & dropout = 0.3       horse       0.439754       0.573   \n",
      "8  Augmentation & dropout = 0.3        ship       0.596679       0.503   \n",
      "9  Augmentation & dropout = 0.3       truck       0.441917       0.544   \n",
      "\n",
      "     MLP F1  CNN Precision  CNN Recall    CNN F1  \n",
      "0  0.530573       0.729282       0.924  0.815174  \n",
      "1  0.556225       0.932029       0.905  0.918316  \n",
      "2  0.256548       0.791860       0.681  0.732258  \n",
      "3  0.311330       0.715832       0.529  0.608396  \n",
      "4  0.389158       0.809231       0.789  0.798987  \n",
      "5  0.320000       0.670962       0.781  0.721811  \n",
      "6  0.470343       0.872449       0.855  0.863636  \n",
      "7  0.497612       0.818786       0.863  0.840312  \n",
      "8  0.545849       0.914494       0.877  0.895355  \n",
      "9  0.487674       0.872939       0.900  0.886263  \n"
     ]
    }
   ],
   "source": [
    "df_classes = compare_per_class_metrics(\n",
    "    mlp_metrics_path=mlp_metrics,\n",
    "    cnn_metrics_path=cnn_metrics,\n",
    "    experiment_name=\"Augmentation & dropout = 0.3\",\n",
    "    class_names=class_names,\n",
    "    save_dir=\"./comparative_tables_DROPOUT_2\"\n",
    ")\n",
    "\n",
    "print(df_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aff26c",
   "metadata": {},
   "source": [
    "# MLP & CNN - Same parameters, augmentation & dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "98d6ec0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparative plots saved in ./comparative_plots_820_2\n"
     ]
    }
   ],
   "source": [
    "mlp_path = './experiments_Matched_3/MLP_820_128_dr0.0_augTrue_20251102_144204/history.json'\n",
    "cnn_path = './experiments_Matched_3/CNN_820_128_dr0.0_augTrue_20251102_152915/history.json'\n",
    "save_dir = './comparative_plots_820_2'\n",
    "exp_name = 'Same parameters ~ 820K'\n",
    "plot_comparative_curves(mlp_path, cnn_path, save_dir, exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e60e27fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tabla comparativa guardada en: ./comparative_tables_820_2/Same parameters ~ 820K_comparison.csv\n",
      "ðŸ“„ Tabla LaTeX guardada en: ./comparative_tables_820_2/Same parameters ~ 820K_comparison.tex\n",
      "               Experiment Model  Test Accuracy  Test Loss  Precision  Recall  \\\n",
      "0  Same parameters ~ 820K   MLP         0.4955   1.394365   0.500221  0.4955   \n",
      "1  Same parameters ~ 820K   CNN         0.8362   0.496637   0.837701  0.8362   \n",
      "\n",
      "   F1 Score  Trainable Params  Total Params  Inference Time (s)  \\\n",
      "0  0.491134            820874        820874            0.747021   \n",
      "1  0.836247            824651        824651            0.862726   \n",
      "\n",
      "   Inference Speed (samples/s)  Final Epoch  Mean Epoch Time (s)  \\\n",
      "0                 13386.505249           50            11.161522   \n",
      "1                 11591.169241           50            13.598976   \n",
      "\n",
      "   Total Training Time (s)  Best Val Accuracy  \n",
      "0               558.239923              50.18  \n",
      "1               680.130645              83.78  \n"
     ]
    }
   ],
   "source": [
    "mlp_metrics = \"./experiments_Matched_3/MLP_820_128_dr0.0_augTrue_20251102_144204/metrics.json\"\n",
    "cnn_metrics = \"./experiments_Matched_3/CNN_820_128_dr0.0_augTrue_20251102_152915/metrics.json\"\n",
    "\n",
    "df = compare_models(\n",
    "    mlp_metrics_path=mlp_metrics,\n",
    "    cnn_metrics_path=cnn_metrics,\n",
    "    experiment_name='Same parameters ~ 820K',\n",
    "    save_dir=\"./comparative_tables_820_2\"\n",
    ")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a22cc3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MÃ©tricas por clase guardadas en: ./comparative_tables_820_2/Same parameters ~ 820K_per_class_metrics.csv\n",
      "ðŸ“„ Tabla LaTeX guardada en: ./comparative_tables_820_2/Same parameters ~ 820K_per_class_metrics.tex\n",
      "               Experiment       Class  MLP Precision  MLP Recall    MLP F1  \\\n",
      "0  Same parameters ~ 820K    airplane       0.605923       0.532  0.566560   \n",
      "1  Same parameters ~ 820K  automobile       0.552833       0.722  0.626193   \n",
      "2  Same parameters ~ 820K        bird       0.437406       0.290  0.348767   \n",
      "3  Same parameters ~ 820K         cat       0.310967       0.431  0.361274   \n",
      "4  Same parameters ~ 820K        deer       0.426265       0.396  0.410575   \n",
      "5  Same parameters ~ 820K         dog       0.442573       0.289  0.349667   \n",
      "6  Same parameters ~ 820K        frog       0.549749       0.547  0.548371   \n",
      "7  Same parameters ~ 820K       horse       0.494942       0.636  0.556674   \n",
      "8  Same parameters ~ 820K        ship       0.666667       0.576  0.618026   \n",
      "9  Same parameters ~ 820K       truck       0.514890       0.536  0.525233   \n",
      "\n",
      "   CNN Precision  CNN Recall    CNN F1  \n",
      "0       0.850656       0.843  0.846811  \n",
      "1       0.949408       0.882  0.914463  \n",
      "2       0.760788       0.811  0.785092  \n",
      "3       0.685024       0.709  0.696806  \n",
      "4       0.803311       0.825  0.814011  \n",
      "5       0.821634       0.714  0.764045  \n",
      "6       0.855787       0.902  0.878286  \n",
      "7       0.884381       0.872  0.878147  \n",
      "8       0.890411       0.910  0.900099  \n",
      "9       0.875612       0.894  0.884711  \n"
     ]
    }
   ],
   "source": [
    "df_classes = compare_per_class_metrics(\n",
    "    mlp_metrics_path=mlp_metrics,\n",
    "    cnn_metrics_path=cnn_metrics,\n",
    "    experiment_name=\"Same parameters ~ 820K\",\n",
    "    class_names=class_names,\n",
    "    save_dir=\"./comparative_tables_820_2\"\n",
    ")\n",
    "\n",
    "print(df_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988c25d9",
   "metadata": {},
   "source": [
    "# MLP & CNN - Same parameters, augmentation & dropout = 0.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5adde974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparative plots saved in ./comparative_plots_820_DROPOUT_2\n"
     ]
    }
   ],
   "source": [
    "mlp_path = './experiments_Matched_2_Dropout/MLP_820_128_dr0.3_augTrue_20251031_165043/history.json'\n",
    "cnn_path = './experiments_Matched_2_Dropout/CNN_820_128_dr0.3_augTrue_20251031_165043/history.json'\n",
    "save_dir = './comparative_plots_820_DROPOUT_2'\n",
    "exp_name = 'Same parameters ~ 820K - dropout = 0.3'\n",
    "plot_comparative_curves(mlp_path, cnn_path, save_dir, exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9a369f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tabla comparativa guardada en: ./comparative_tables_820_DROPOUT_2/Same parameters ~ 820K - dropout = 0.3_comparison.csv\n",
      "ðŸ“„ Tabla LaTeX guardada en: ./comparative_tables_820_DROPOUT_2/Same parameters ~ 820K - dropout = 0.3_comparison.tex\n",
      "                               Experiment Model  Test Accuracy  Test Loss  \\\n",
      "0  Same parameters ~ 820K - dropout = 0.3   MLP         0.4962   1.409175   \n",
      "1  Same parameters ~ 820K - dropout = 0.3   CNN         0.8299   0.511080   \n",
      "\n",
      "   Precision  Recall  F1 Score  Trainable Params  Total Params  \\\n",
      "0   0.490849  0.4962  0.488912            820874        820874   \n",
      "1   0.832331  0.8299  0.829745            824651        824651   \n",
      "\n",
      "   Inference Time (s)  Inference Speed (samples/s)  Final Epoch  \\\n",
      "0            0.670154                 14921.946515           50   \n",
      "1            1.025710                  9749.343350           50   \n",
      "\n",
      "   Mean Epoch Time (s)  Total Training Time (s)  Best Val Accuracy  \n",
      "0            10.859155               543.115424              50.02  \n",
      "1            13.916918               696.089328              83.65  \n"
     ]
    }
   ],
   "source": [
    "mlp_metrics = \"./experiments_Matched_2_Dropout/MLP_820_128_dr0.3_augTrue_20251031_165043/metrics.json\"\n",
    "cnn_metrics = \"./experiments_Matched_2_Dropout/CNN_820_128_dr0.3_augTrue_20251031_165043/metrics.json\"\n",
    "\n",
    "df = compare_models(\n",
    "    mlp_metrics_path=mlp_metrics,\n",
    "    cnn_metrics_path=cnn_metrics,\n",
    "    experiment_name='Same parameters ~ 820K - dropout = 0.3',\n",
    "    save_dir=\"./comparative_tables_820_DROPOUT_2\"\n",
    ")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "459a7eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MÃ©tricas por clase guardadas en: ./comparative_tables_820_DROPOUT_2/Same parameters ~ 820K - dropout = 0.3_per_class_metrics.csv\n",
      "ðŸ“„ Tabla LaTeX guardada en: ./comparative_tables_820_DROPOUT_2/Same parameters ~ 820K - dropout = 0.3_per_class_metrics.tex\n",
      "                               Experiment       Class  MLP Precision  \\\n",
      "0  Same parameters ~ 820K - dropout = 0.3    airplane       0.563017   \n",
      "1  Same parameters ~ 820K - dropout = 0.3  automobile       0.598299   \n",
      "2  Same parameters ~ 820K - dropout = 0.3        bird       0.435440   \n",
      "3  Same parameters ~ 820K - dropout = 0.3         cat       0.370370   \n",
      "4  Same parameters ~ 820K - dropout = 0.3        deer       0.414258   \n",
      "5  Same parameters ~ 820K - dropout = 0.3         dog       0.451910   \n",
      "6  Same parameters ~ 820K - dropout = 0.3        frog       0.437063   \n",
      "7  Same parameters ~ 820K - dropout = 0.3       horse       0.530631   \n",
      "8  Same parameters ~ 820K - dropout = 0.3        ship       0.595196   \n",
      "9  Same parameters ~ 820K - dropout = 0.3       truck       0.512311   \n",
      "\n",
      "   MLP Recall    MLP F1  CNN Precision  CNN Recall    CNN F1  \n",
      "0       0.545  0.553862       0.833822       0.853  0.843302  \n",
      "1       0.633  0.615160       0.869167       0.950  0.907788  \n",
      "2       0.317  0.366898       0.825287       0.718  0.767914  \n",
      "3       0.270  0.312319       0.682711       0.695  0.688801  \n",
      "4       0.430  0.421982       0.829960       0.820  0.824950  \n",
      "5       0.343  0.389994       0.720696       0.787  0.752390  \n",
      "6       0.625  0.514403       0.941109       0.815  0.873526  \n",
      "7       0.589  0.558294       0.843931       0.876  0.859666  \n",
      "8       0.669  0.629944       0.869850       0.929  0.898453  \n",
      "9       0.541  0.526265       0.906780       0.856  0.880658  \n"
     ]
    }
   ],
   "source": [
    "df_classes = compare_per_class_metrics(\n",
    "    mlp_metrics_path=mlp_metrics,\n",
    "    cnn_metrics_path=cnn_metrics,\n",
    "    experiment_name='Same parameters ~ 820K - dropout = 0.3',\n",
    "    class_names=class_names,\n",
    "    save_dir=\"./comparative_tables_820_DROPOUT_2\"\n",
    ")\n",
    "\n",
    "print(df_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
